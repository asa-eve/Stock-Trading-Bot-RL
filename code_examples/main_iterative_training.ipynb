{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7702c32-15d9-401c-a024-d4ce7a4073e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0ca16-c22e-45df-acbd-babd4a809988",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/asa-eve/Trading_Bot_RL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca3e445-920c-48f4-a894-67fad97da74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adm.000\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Path to directory above Trading_Bot_RL on 1 level\n",
    "sys.path.insert(0, f'{os.path.dirname(os.getcwd())}')\n",
    "\n",
    "from trading_bot_rl.agent import *\n",
    "from trading_bot_rl.env import *\n",
    "\n",
    "from trading_bot_rl.functions.general import *\n",
    "from trading_bot_rl.functions.callbacks import *\n",
    "from trading_bot_rl.functions.env_functions import *\n",
    "from trading_bot_rl.functions.data_preprocessing import *\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "def env_kwargs_reinit():\n",
    "    return {\n",
    "    \"hmax\": kwarg_hmax,\n",
    "    \"initial_amount\": kwarg_initial_amount,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list, # buy_cost_list[0],\n",
    "    \"sell_cost_pct\": sell_cost_list, #sell_cost_list[0],\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": kwarg_reward_scaling,\n",
    "    \"make_plots\": MAKE_PLOTS,\n",
    "    \"print_verbosity\": VERBOSITY_PRINT,\n",
    "    \"discrete_action_space\": discrete_action_space,\n",
    "}\n",
    "\n",
    "def callback(chosen_callback):\n",
    "    if chosen_callback == 'tensorboard':\n",
    "        return TensorboardCallback\n",
    "    elif chosen_callback == 'eval':\n",
    "        return eval_callback\n",
    "    elif chosen_callback == 'checkpoint':\n",
    "        return checkpoint_callback\n",
    "    \n",
    "def print_training_info():\n",
    "    print('---')\n",
    "    print('    Training with next initial parameters:')\n",
    "    print('')\n",
    "    print(f\"          Mode - {'normal'*(1-only_forecasts_data) + (' |' * ((df_name_forecasts != None) and (only_forecasts_data != True))) + ' forecasted'*(df_name_forecasts!=None)}\")\n",
    "    print('')\n",
    "    print(f'       Number of Stocks to trade  - {kwarg_hmax}')\n",
    "    print(f'       Initial money amount       - {kwarg_initial_amount}')\n",
    "    print(f'       Commision                  - {kwarg_buy_sell_cost}')\n",
    "    print(f'       Env reward scaling         - {kwarg_reward_scaling}')\n",
    "    print(f\"       discrete action space      - {('No' * (1 - discrete_action_space)) + ('Yes' * (discrete_action_space))}\")\n",
    "    print('')\n",
    "    print(f\"   Training loop from {start_training_episode} to {end_training_episode - 1} (step = {step_training_episodes})\")\n",
    "    print('')\n",
    "    print(f\"       RL model                   - {model_name}\")\n",
    "    print(f\"       Fixed seed                 - {('No' * (1 - fixed_seed)) + ('Yes' * (fixed_seed))}\")\n",
    "    print(f\"       Device                     - {algorithm_parameters['device']}\")\n",
    "    print('')\n",
    "    print(f\"   Data processing info\")\n",
    "    print('')\n",
    "    print(f\"       Valid + Trade              - {int(test_and_valid_pct * 100)} %\") if valid_split else print(f\"       Trade                      - {int(test_and_valid_pct * 100)} %\")\n",
    "    print(f\"       Tech Indicators usage      - {('No' * (1 - dict_args['tech_indicators_usage'])) + ('Yes' * (dict_args['tech_indicators_usage']))}\")\n",
    "    print('')\n",
    "    print('---')\n",
    "    for i in range(3): print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9375b-f712-4967-8121-f78349dfdb0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3d5068-b704-4ade-9e7d-ae7073b2adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = {}                       # dictionary for 'saving' models\n",
    "last_model_trained = None                 # 'for testing' - takes last trained model as one for test\n",
    "\n",
    "# RL parameters -----------------\n",
    "discrete_action_space = False             # For discrete 'action_space' in env [21 ~ 0.1 step, 11 ~ 0.2 step, etc..]\n",
    "if discrete_action_space: \n",
    "    discrete_actions = 11   \n",
    "\n",
    "VERBOSITY_PRINT = 1                       # in 'episodes' \n",
    "#VERBOSE_INFO_TRAINING = False             # verbosity for 'stable baselines training'\n",
    "#VERBOSITY_PRINT = 1000*(1-VERBOSE_INFO_TRAINING) + 1*VERBOSE_INFO_TRAINING\n",
    "MAKE_PLOTS = False\n",
    "\n",
    "chosen_callback = 'tensorboard'           # 'tensorboard', 'eval', 'checkpoint', None\n",
    "if chosen_callback == 'tensorboard': callback_arg = BaseCallback\n",
    "if chosen_callback == 'eval': callback_arg = None\n",
    "if chosen_callback == 'checkpoint': callback_arg = None\n",
    "\n",
    "fixed_seed = True                        # only for replicating results or hyperparameters tuning\n",
    "if fixed_seed: \n",
    "    seed_values = [1]\n",
    "\n",
    "# {\"a2c\": A2C, \"ddpg\": DDPG, \"td3\": TD3, \"sac\": SAC, \"ppo\": PPO, \"trpo\": TRPO, \"lstm_ppo\": RecurrentPPO}\n",
    "model_name = 'lstm_ppo'\n",
    "algorithm_parameters = {\"learning_rate\": 1e-4,\n",
    "                        \"device\": 'cuda',\n",
    "                         }\n",
    "\n",
    "\n",
    "# Threshold parameters -----------------\n",
    "#quantile = None\n",
    "#turbulence_threshold = quantile      # turbulence_threshold_define() to get turbulence\n",
    "#risk_indicator_col = None            # 'vix' column\n",
    "\n",
    "# Env parameters -----------------\n",
    "kwarg_hmax = 100\n",
    "kwarg_initial_amount = 1000000\n",
    "kwarg_reward_scaling = 1e-4              \n",
    "kwarg_buy_sell_cost = 0.001\n",
    "\n",
    "# Iterative Training parameters ------------\n",
    "times_loop_training = 1\n",
    "start_training_episode = 50                                              # start == episode until which to train 'first time'\n",
    "step_training_episodes = start_training_episode                          # step == number of episodes to train afterwards\n",
    "end_training_episode = start_training_episode * times_loop_training + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "454da8be-b2b7-44b9-b0e2-b858d4b770d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "    Training with next initial parameters:\n",
      "\n",
      "          Mode - normal\n",
      "\n",
      "       Number of Stocks to trade  - 100\n",
      "       Initial money amount       - 1000000\n",
      "       Commision                  - 0.001\n",
      "       Env reward scaling         - 0.0001\n",
      "       discrete action space      - No\n",
      "\n",
      "   Training loop from 50 to 50 (step = 50)\n",
      "\n",
      "       RL model                   - a2c\n",
      "       Fixed seed                 - Yes\n",
      "       Device                     - cuda\n",
      "\n",
      "   Data processing info\n",
      "\n",
      "       Trade                      - 15 %\n",
      "       Tech Indicators usage      - No\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All features (except 'date' are chosen to by RL feature ---> want to remove something ---> drop with pandas)\n",
    "# Must have column names ('date', 'open', 'close', 'volume', 'high', 'low') ---> use pandas rename your df\n",
    "# -----------------------\n",
    "\n",
    "df_names = ['^GSPC_ta_my_features'] # ['all_ta_features', 'filtered_features', '^GSPC_ta_my_features']\n",
    "df_name_forecasts = None            # '_with_forecasts_LSTM_1_120'\n",
    "only_forecasts_data = False\n",
    "unwanted_features = ['date', 'tic']\n",
    "encode_normalize_data = True\n",
    "\n",
    "path_to_datasets = (os.path.dirname(os.getcwd())+'\\\\datasets\\\\').replace(\"\\\\\",\"/\")\n",
    "path_to_models = (os.path.dirname(os.getcwd())+'\\\\trained_models\\\\').replace(\"\\\\\",\"/\")\n",
    "\n",
    "test_and_valid_pct = 0.15\n",
    "valid_split = False\n",
    "BOOL_TO_INT = True\n",
    "\n",
    "dict_args={\n",
    "            \"test_and_valid_pct\": test_and_valid_pct,\n",
    "            \"tic_name\": 'SPY',\n",
    "            \"valid_split\": valid_split,\n",
    "            \"BOOL_TO_INT\": BOOL_TO_INT,\n",
    "            \"tech_indicators_usage\": False,\n",
    "            \"use_vix\": False,\n",
    "            \"use_turbulence\": False,\n",
    "            \"user_defined_feature\": False,\n",
    "}\n",
    "\n",
    "print_training_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4898b-efc0-4ebc-8872-aa355346782d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Iterative Training Process\n",
    "- works with - 'data' & (data, data_with_forecasts)\n",
    "- iterated through all df's (in df_names)\n",
    "    - iterated through all seeds (in seed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24516010-9188-404e-8124-bd386f13f56f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  1993-01-29   2018-07-18\n",
      "trade  2018-07-19   2023-01-13\n",
      "Stock Dimension: 1, State Space: 126, State Space Forecasts: None\n",
      "{'learning_rate': 0.0001, 'device': 'cuda'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adm.000\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 6413, episode: 3\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1663585.04\n",
      "total_reward: 663585.04\n",
      "total_cost: 468263.37\n",
      "total_trades: 6253\n",
      "Sharpe: 0.232\n",
      "=================================\n",
      "day: 6413, episode: 4\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3543965.25\n",
      "total_reward: 2543965.25\n",
      "total_cost: 493029.96\n",
      "total_trades: 6300\n",
      "Sharpe: 0.606\n",
      "=================================\n",
      "day: 6413, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2141849.30\n",
      "total_reward: 1141849.30\n",
      "total_cost: 472247.97\n",
      "total_trades: 6268\n",
      "Sharpe: 0.315\n",
      "=================================\n",
      "day: 6413, episode: 6\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1852345.31\n",
      "total_reward: 852345.31\n",
      "total_cost: 468974.52\n",
      "total_trades: 6302\n",
      "Sharpe: 0.254\n",
      "=================================\n",
      "day: 6413, episode: 7\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3984813.68\n",
      "total_reward: 2984813.68\n",
      "total_cost: 468655.46\n",
      "total_trades: 6349\n",
      "Sharpe: 0.451\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m agent \u001b[38;5;241m=\u001b[39m DRLAgent(env \u001b[38;5;241m=\u001b[39m env_train)\n\u001b[0;32m     66\u001b[0m model \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_model(model_name\u001b[38;5;241m=\u001b[39mmodel_name, model_kwargs \u001b[38;5;241m=\u001b[39m algorithm_parameters, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, seed\u001b[38;5;241m=\u001b[39mseed_value) \u001b[38;5;28;01mif\u001b[39;00m (k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m trained\n\u001b[1;32m---> 67\u001b[0m trained \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TRAINING_STEPS_FOR_1_TRIAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchosen_callback\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback_arg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# CALCULATING sharpe on 'valid / test'\u001b[39;00m\n\u001b[0;32m     73\u001b[0m env_trade_gym, env_trade \u001b[38;5;241m=\u001b[39m env_reinit(valid, env_kwargs_reinit()) \u001b[38;5;28;01mif\u001b[39;00m valid_split \u001b[38;5;28;01melse\u001b[39;00m env_reinit(trade, env_kwargs_reinit())\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Trading_Bot_RL\\trading_bot_rl\\agent.py:60\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[1;34m(self, model, tb_log_name, total_timesteps, callback)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, callback\u001b[38;5;241m=\u001b[39mTensorboardCallback()):\n\u001b[1;32m---> 60\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:190\u001b[0m, in \u001b[0;36mA2C.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[0;32m    183\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    188\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    189\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfA2C:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:246\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    243\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 246\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:174\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m    172\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 174\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:171\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:57\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 57\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\shimmy\\openai_gym_compatibility.py:255\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Trading_Bot_RL\\trading_bot_rl\\env.py:354\u001b[0m, in \u001b[0;36mStockTradingEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mtic\u001b[38;5;241m.\u001b[39munique()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturbulence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrisk_indicator_col]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m end_total_asset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m    357\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m1\u001b[39m : (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) : (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[0;32m    359\u001b[0m )\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masset_memory\u001b[38;5;241m.\u001b[39mappend(end_total_asset)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Trading_Bot_RL\\trading_bot_rl\\env.py:466\u001b[0m, in \u001b[0;36mStockTradingEnv._update_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;66;03m# for multiple stock\u001b[39;00m\n\u001b[0;32m    468\u001b[0m         state \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    469\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    470\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclose\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    478\u001b[0m             )\n\u001b[0;32m    479\u001b[0m         )\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;66;03m# for single stock\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\series.py:2088\u001b[0m, in \u001b[0;36mSeries.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2030\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[0;32m   2031\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[0;32m   2087\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\base.py:989\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    987\u001b[0m             result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(result)\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:440\u001b[0m, in \u001b[0;36munique\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    437\u001b[0m htable, values \u001b[38;5;241m=\u001b[39m _get_hashtable_algo(values)\n\u001b[0;32m    439\u001b[0m table \u001b[38;5;241m=\u001b[39m htable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m--> 440\u001b[0m uniques \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For any 'data' in list\n",
    "for df_name in df_names:\n",
    "\n",
    "# Data unpackage + preprocessing\n",
    "    df_main_file = path_to_datasets+f\"{df_name}.csv\"\n",
    "    df_forecasts_file = path_to_datasets+f\"{df_name+df_name_forecasts}.csv\" if (df_name_forecasts != None) else (None)\n",
    "    train_main, valid_main, trade_main, train_forecasts, valid_forecasts, trade_forecasts = data_read_preprocessing_singleTIC(df_main_file, df_forecasts_file, **dict_args)\n",
    "\n",
    "# Defining features (of state_space)\n",
    "    INDICATORS_MAIN = train_main.columns.tolist()\n",
    "    for feature in unwanted_features: \n",
    "        if feature in INDICATORS_MAIN: INDICATORS_MAIN.remove(feature)\n",
    "    if df_forecasts_file != None:\n",
    "        INDICATORS_FORECASTS = train_forecasts.columns.tolist()\n",
    "        for feature in unwanted_features: \n",
    "            if feature in INDICATORS_FORECASTS: INDICATORS_FORECASTS.remove(feature)\n",
    "\n",
    "# RL Env parameters defining\n",
    "    stock_dimension = len(train_main.tic.unique())\n",
    "    state_space_main = 1 + 2*stock_dimension + len(INDICATORS_MAIN)*stock_dimension\n",
    "    state_space_forecasts = 1 + 2*stock_dimension + len(INDICATORS_FORECASTS)*stock_dimension if (df_forecasts_file != None) else (None)\n",
    "    buy_cost_list = sell_cost_list = [kwarg_buy_sell_cost] * stock_dimension\n",
    "    num_stock_shares = [0] * stock_dimension\n",
    "    print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space_main}, State Space Forecasts: {state_space_forecasts}\")\n",
    "\n",
    "    sharpe_forecastFalse = []\n",
    "    sharpe_forecastTrue = []\n",
    "\n",
    "# Main loop (for 'seeds' -> iteratively train 'data' (and if there 'data_with_forecasts') -> save each model and calculate sharpe ->  )\n",
    "    for seed_value in (seed_values*fixed_seed + (1 - fixed_seed) * [None]):\n",
    "        if fixed_seed: set_seed(seed_value)\n",
    "\n",
    "        # Training only 'data' or with 'data_with_forecasts'\n",
    "        is_forecast_list = [False, True] if (df_name_forecasts != None and (not only_forecasts_data)) \\\n",
    "                                                                        else ([False] if (not only_forecasts_data) else ([True] if (only_forecasts_data and df_name_forecasts != None) else []))\n",
    "\n",
    "        for is_forecast in is_forecast_list:\n",
    "\n",
    "            state_space = state_space_forecasts if is_forecast else state_space_main\n",
    "            INDICATORS = INDICATORS_FORECASTS if is_forecast else INDICATORS_MAIN\n",
    "            train = train_forecasts if is_forecast else train_main\n",
    "            trade = trade_forecasts if is_forecast else trade_main\n",
    "            if valid_split: valid = valid_forecasts if is_forecast else valid_main\n",
    "\n",
    "            # Define 'env' parapeters\n",
    "            env_kwargs = env_kwargs_reinit()\n",
    "\n",
    "            # Counter for training (if k=0 -> no model, initialize) ELSE (model = trained)\n",
    "            k=0\n",
    "\n",
    "            # Iterative training loop\n",
    "            for i in range(start_training_episode,end_training_episode,step_training_episodes):\n",
    "                NUM_EPISODES = i \n",
    "                EPISODE_LENGTH = len(train)\n",
    "                NUM_TRAINING_STEPS_FOR_1_TRIAL = NUM_EPISODES * EPISODE_LENGTH \n",
    "\n",
    "                name_of_the_save_file = f\"{path_to_models}{model_name}_techindicators{dict_args['tech_indicators_usage']}_vix{dict_args['use_vix']}_turbulence{dict_args['use_turbulence']}\\\\{df_name}_{model_name}_lr{algorithm_parameters['learning_rate']}_Forecast{is_forecast}_Seed{str(seed_value)*fixed_seed + str(None)*(1-fixed_seed)}_Episodes{NUM_EPISODES}_Sharpe{valid_split*'Valid' + (1 - valid_split)*'Test'}{int(test_and_valid_pct*100)}\"\n",
    "                name_of_the_save_file_zip = name_of_the_save_file + \".zip\"\n",
    "\n",
    "                # MODEL TRAINING\n",
    "                if not os.path.exists(name_of_the_save_file_zip):\n",
    "\n",
    "                    # MODEL TRAINING\n",
    "                    e_train_gym, env_train = env_reinit(train, env_kwargs_reinit())\n",
    "                    agent = DRLAgent(env = env_train)\n",
    "                    model = agent.get_model(model_name=model_name, model_kwargs = algorithm_parameters, verbose=0, seed=seed_value) if (k == 0) else trained\n",
    "                    trained = agent.train_model(model=model, \n",
    "                                                    tb_log_name=model_name,\n",
    "                                                    total_timesteps=NUM_TRAINING_STEPS_FOR_1_TRIAL,\n",
    "                                                    callback=callback(chosen_callback)(callback_arg))\n",
    "\n",
    "                    # CALCULATING sharpe on 'valid / test'\n",
    "                    env_trade_gym, env_trade = env_reinit(valid, env_kwargs_reinit()) if valid_split else env_reinit(trade, env_kwargs_reinit())\n",
    "                    df_account_value, df_actions = DRLAgent.prediction(model=trained, environment = env_trade_gym)\n",
    "                    sharpe = calculate_sharpe(df_account_value)\n",
    "                    sharpe_forecastTrue.append(sharpe) if is_forecast else sharpe_forecastFalse.append(sharpe)\n",
    "\n",
    "                    # MODEL SAVING\n",
    "                    trained.save(name_of_the_save_file_zip)\n",
    "\n",
    "                    print('Forecasts = ', is_forecast, '| Seed = ', seed_value, '| Num episodes = ', i, f'| Sharpe {valid_split*\"Valid\" + (1 - valid_split)*\"Test\"} = ', sharpe)\n",
    "                    print('---')\n",
    "                    k+=1\n",
    "\n",
    "                # MODEL LOADING + sharpe calculating\n",
    "                else:\n",
    "                    print('Model already exists')\n",
    "                    k+=1\n",
    "\n",
    "                    e_train_gym, env_train = env_reinit(train, env_kwargs_reinit())\n",
    "                    agent = DRLAgent(env = env_train)\n",
    "                    model = agent.get_model(model_name=model_name, model_kwargs = algorithm_parameters, verbose=0, seed=seed_value)\n",
    "                    trained = model.load(name_of_the_save_file, env = env_train)\n",
    "\n",
    "                    env_trade_gym, env_trade = env_reinit(valid, env_kwargs_reinit()) if valid_split else env_reinit(trade, env_kwargs_reinit())\n",
    "                    df_account_value, df_actions = DRLAgent.prediction(model=trained, environment = env_trade_gym)\n",
    "                    sharpe = calculate_sharpe(df_account_value)\n",
    "                    sharpe_forecastTrue.append(sharpe) if is_forecast else sharpe_forecastFalse.append(sharpe)\n",
    "\n",
    "                    print('Forecasts = ', is_forecast, '| Seed = ', seed_value, '| Num episodes = ', i, f'| Sharpe {valid_split*\"Valid\" + (1 - valid_split)*\"Test\"} = ', sharpe)\n",
    "                    print('---')\n",
    "\n",
    "\n",
    "        print('------')\n",
    "        print(df_name)\n",
    "        print('Seed average no forecast = ', np.mean(sharpe_forecastFalse))\n",
    "        print('Seed average with forecast = ', np.mean(sharpe_forecastTrue))\n",
    "        print('------')\n",
    "        print('No forecast = ', sharpe_forecastFalse)\n",
    "        print('With forecast =', sharpe_forecastTrue)\n",
    "        print('------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
