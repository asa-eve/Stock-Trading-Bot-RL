{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7702c32-15d9-401c-a024-d4ce7a4073e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0ca16-c22e-45df-acbd-babd4a809988",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/asa-eve/Trading_Bot_RL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca3e445-920c-48f4-a894-67fad97da74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adm.000\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Path to directory above Trading_Bot_RL on 1 level\n",
    "sys.path.insert(0, f'{os.path.dirname(os.getcwd())}')\n",
    "\n",
    "from trading_bot_rl.agent import *\n",
    "from trading_bot_rl.env import *\n",
    "\n",
    "from trading_bot_rl.functions.general import *\n",
    "from trading_bot_rl.functions.callbacks import *\n",
    "from trading_bot_rl.functions.env_functions import *\n",
    "from trading_bot_rl.functions.data_preprocessing import *\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "\n",
    "def env_kwargs_reinit():\n",
    "    return {\n",
    "    \"hmax\": kwarg_hmax,\n",
    "    \"initial_amount\": kwarg_initial_amount,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list, # buy_cost_list[0],\n",
    "    \"sell_cost_pct\": sell_cost_list, #sell_cost_list[0],\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": kwarg_reward_scaling,\n",
    "    \"make_plots\": MAKE_PLOTS,\n",
    "    \"print_verbosity\": VERBOSITY_PRINT,\n",
    "    \"discrete_action_space\": discrete_action_space,\n",
    "    \"seed\": seed_value\n",
    "}\n",
    "    \n",
    "def print_training_info():\n",
    "    print('---')\n",
    "    print('    Training with next initial parameters:')\n",
    "    print('')\n",
    "    print(f\"          Mode - {'normal'*(1-only_forecasts_data) + (' |' * ((df_name_forecasts != None) and (only_forecasts_data != True))) + ' forecasted'*(df_name_forecasts!=None)}\")\n",
    "    print('')\n",
    "    print(f'       Number of Stocks to trade  - {kwarg_hmax}')\n",
    "    print(f'       Initial money amount       - {kwarg_initial_amount}')\n",
    "    print(f'       Commision                  - {kwarg_buy_sell_cost}')\n",
    "    print(f'       Env reward scaling         - {kwarg_reward_scaling}')\n",
    "    print(f\"       discrete action space      - {('No' * (1 - discrete_action_space)) + ('Yes' * (discrete_action_space))}\")\n",
    "    print('')\n",
    "    print(f\"   Training loop from {start_training_episode} to {end_training_episode - 1} (step = {step_training_episodes})\")\n",
    "    print('')\n",
    "    print(f\"       RL model                   - {model_name}\")\n",
    "    print(f\"       Fixed seed                 - {('No' * (1 - fixed_seed)) + ('Yes' * (fixed_seed))}\")\n",
    "    print(f\"       Device                     - {algorithm_parameters['device']}\")\n",
    "    print('')\n",
    "    print(f\"   Data processing info\")\n",
    "    print('')\n",
    "    print(f\"       Valid + Trade              - {int(test_and_valid_pct * 100)} %\") if valid_split else print(f\"       Trade                      - {int(test_and_valid_pct * 100)} %\")\n",
    "    print(f\"       Tech Indicators usage      - {('No' * (1 - dict_args['tech_indicators_usage'])) + ('Yes' * (dict_args['tech_indicators_usage']))}\")\n",
    "    print(f\"       'date' cyclic encoding     - {('No' * (1 - encode_normalize_data)) + ('Yes' * (encode_normalize_data))}\")\n",
    "    print(f\"       Normalization              - {('No' * (1 - (dict_args['scaler'] == None))) + ('Yes' * (dict_args['scaler'] != None))}\")\n",
    "    print('')\n",
    "    print('---')\n",
    "    for i in range(3): print('')\n",
    "    \n",
    "# --------------------    \n",
    "# Output functions in case needed\n",
    "#with open(path_f + 'monkey.txt', 'w') as file:\n",
    "    # Redirect the standard output to the file\n",
    "#    sys.stdout = file\n",
    "\n",
    "    # Print some output to the console\n",
    "#    print('This is some output')\n",
    "# Reset the standard output to the console\n",
    "#sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9375b-f712-4967-8121-f78349dfdb0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3d5068-b704-4ade-9e7d-ae7073b2adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold parameters -----------------\n",
    "#quantile = None\n",
    "#turbulence_threshold = quantile      # turbulence_threshold_define() to get turbulence\n",
    "#risk_indicator_col = None            # 'vix' column\n",
    "\n",
    "# Env parameters -----------------\n",
    "kwarg_hmax = 100\n",
    "kwarg_initial_amount = 1000000\n",
    "kwarg_reward_scaling = 1e-3         # 3*1e-5      \n",
    "kwarg_buy_sell_cost = 0.001\n",
    "\n",
    "# RL parameters -----------------\n",
    "VERBOSITY_PRINT = 1                       # in 'episodes' \n",
    "MAKE_PLOTS = False\n",
    "#VERBOSE_INFO_TRAINING = False             # verbosity for 'stable baselines training'\n",
    "\n",
    "discrete_action_space = False             # For discrete 'action_space' in env [21 ~ 0.1 step, 11 ~ 0.2 step, etc..]\n",
    "if discrete_action_space: \n",
    "    discrete_actions = 11   \n",
    "\n",
    "# ______________\n",
    "chosen_callback = 'tensorboard'           # 'tensorboard', 'eval', 'checkpoint', None\n",
    "if chosen_callback == 'eval':             # evaluate model perfomance every 'eval_freq' steps (on valid/test data)\n",
    "    eval_freq_ep = 5\n",
    "    n_eval_episodes = 1\n",
    "    \n",
    "# ______________\n",
    "fixed_seed = True                        # only for replicating results or hyperparameters tuning\n",
    "if fixed_seed: \n",
    "    seed_values = [1]\n",
    "\n",
    "# ______________\n",
    "# {\"a2c\": A2C, \"ddpg\": DDPG, \"td3\": TD3, \"sac\": SAC, \"ppo\": PPO, \"trpo\": TRPO, \"lstm_ppo\": RecurrentPPO}\n",
    "model_name = 'lstm_ppo'\n",
    "algorithm_parameters = {\"learning_rate\": 1e-3,\n",
    "                        \"device\": 'cuda',\n",
    "                        \"gamma\": 0.9,\n",
    "                        \"clip_range_vf\": 0.5,\n",
    "                        \"clip_range\": 0.1,\n",
    "                        \"batch_size\": 256,\n",
    "                        \"n_steps\": 256,\n",
    "                        \"n_epochs\": 35,\n",
    "                         }\n",
    "\n",
    "# Iterative Training parameters ------------\n",
    "times_loop_training = 40\n",
    "start_training_episode = 50                                              # start == episode until which to train 'first time'\n",
    "step_training_episodes = start_training_episode                          # step == number of episodes to train afterwards\n",
    "end_training_episode = start_training_episode * times_loop_training + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454da8be-b2b7-44b9-b0e2-b858d4b770d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "    Training with next initial parameters:\n",
      "\n",
      "          Mode -  forecasted\n",
      "\n",
      "       Number of Stocks to trade  - 100\n",
      "       Initial money amount       - 1000000\n",
      "       Commision                  - 0.001\n",
      "       Env reward scaling         - 0.001\n",
      "       discrete action space      - No\n",
      "\n",
      "   Training loop from 50 to 2000 (step = 50)\n",
      "\n",
      "       RL model                   - lstm_ppo\n",
      "       Fixed seed                 - Yes\n",
      "       Device                     - cuda\n",
      "\n",
      "   Data processing info\n",
      "\n",
      "       Trade                      - 15 %\n",
      "       Tech Indicators usage      - No\n",
      "       'date' cyclic encoding     - Yes\n",
      "       Normalization              - \n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_names = ['test_filtered_features_features58'] # ['all_ta_features', 'filtered_features', '^GSPC_ta_my_features']\n",
    "df_name_forecasts = '_with_forecasts_LSTM_1_120'            # '_with_forecasts_LSTM_1_120'\n",
    "\n",
    "only_forecasts_data = True\n",
    "unwanted_features = ['date', 'tic']    # unwanted in INDICATORS as features\n",
    "\n",
    "test_and_valid_pct = 0.15\n",
    "valid_split = False\n",
    "BOOL_TO_INT = True\n",
    "\n",
    "dict_args={\n",
    "            \"test_and_valid_pct\": test_and_valid_pct,\n",
    "            \"tic_name\": 'SPY',\n",
    "            \"valid_split\": valid_split,\n",
    "            \"BOOL_TO_INT\": BOOL_TO_INT,\n",
    "            \"tech_indicators_usage\": False,\n",
    "            \"use_vix\": False,\n",
    "            \"use_turbulence\": False,\n",
    "            \"user_defined_feature\": False,\n",
    "}\n",
    "\n",
    "encode_normalize_data = True\n",
    "if encode_normalize_data: dict_args[\"scaler\"] = None # 'MinMax', 'Standard', 'Robust', None\n",
    "\n",
    "path_to_datasets = (os.path.dirname(os.getcwd())+'\\\\datasets\\\\').replace(\"\\\\\",\"/\")\n",
    "path_to_models = (os.path.dirname(os.getcwd())+'\\\\trained_models\\\\').replace(\"\\\\\",\"/\")\n",
    "\n",
    "print_training_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4898b-efc0-4ebc-8872-aa355346782d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Iterative Training Process\n",
    "- works with - 'data' & (data, data_with_forecasts)\n",
    "- iterated through all df's (in df_names)\n",
    "    - iterated through all seeds (in seed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24516010-9188-404e-8124-bd386f13f56f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  1999-03-19   2019-06-06\n",
      "trade  2019-06-07   2022-12-27\n",
      "train  1999-03-19   2019-06-06\n",
      "trade  2019-06-07   2022-12-27\n",
      "Stock Dimension: 1, State Space: 62, State Space Forecasts: 64\n",
      "------\n",
      "------\n",
      "LOOP NUMBER 1 --- 0 EPISODES TRAINED\n",
      "------\n",
      "------\n",
      "{'learning_rate': 0.001, 'device': 'cuda', 'gamma': 0.9, 'clip_range_vf': 0.5, 'clip_range': 0.1, 'batch_size': 256, 'n_steps': 256, 'n_epochs': 35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adm.000\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 5086, episode: 1\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1915756.12\n",
      "total_reward: 915756.12\n",
      "total_cost: 381437.92\n",
      "total_trades: 4436\n",
      "total_agent_reward: 915.7561205233974\n",
      "Sharpe: 0.296\n",
      "=================================\n",
      "day: 5086, episode: 2\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1202475.66\n",
      "total_reward: 202475.66\n",
      "total_cost: 291614.19\n",
      "total_trades: 3796\n",
      "total_agent_reward: 202.4756559402029\n",
      "Sharpe: 0.138\n",
      "=================================\n",
      "day: 5086, episode: 3\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1059123.68\n",
      "total_reward: 59123.68\n",
      "total_cost: 344177.56\n",
      "total_trades: 4024\n",
      "total_agent_reward: 59.12367646211777\n",
      "Sharpe: 0.095\n",
      "=================================\n",
      "day: 5086, episode: 4\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 991406.43\n",
      "total_reward: -8593.57\n",
      "total_cost: 311523.16\n",
      "total_trades: 3757\n",
      "total_agent_reward: -8.59357000169291\n",
      "Sharpe: 0.054\n",
      "=================================\n",
      "day: 5086, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1311545.37\n",
      "total_reward: 311545.37\n",
      "total_cost: 315495.83\n",
      "total_trades: 3516\n",
      "total_agent_reward: 311.54536939351067\n",
      "Sharpe: 0.171\n",
      "=================================\n",
      "day: 5086, episode: 6\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2087470.99\n",
      "total_reward: 1087470.99\n",
      "total_cost: 357105.99\n",
      "total_trades: 3844\n",
      "total_agent_reward: 1087.4709875701492\n",
      "Sharpe: 0.349\n",
      "=================================\n",
      "day: 5086, episode: 7\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1076673.12\n",
      "total_reward: 76673.12\n",
      "total_cost: 300946.51\n",
      "total_trades: 3486\n",
      "total_agent_reward: 76.67312453626886\n",
      "Sharpe: 0.096\n",
      "=================================\n",
      "day: 5086, episode: 8\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1714313.95\n",
      "total_reward: 714313.95\n",
      "total_cost: 290499.26\n",
      "total_trades: 3698\n",
      "total_agent_reward: 714.3139454401407\n",
      "Sharpe: 0.275\n",
      "=================================\n",
      "day: 5086, episode: 9\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2056157.42\n",
      "total_reward: 1056157.42\n",
      "total_cost: 284788.39\n",
      "total_trades: 3680\n",
      "total_agent_reward: 1056.157420713821\n",
      "Sharpe: 0.339\n",
      "=================================\n",
      "day: 5086, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2026314.38\n",
      "total_reward: 1026314.38\n",
      "total_cost: 248680.09\n",
      "total_trades: 3424\n",
      "total_agent_reward: 1026.3143782214484\n",
      "Sharpe: 0.314\n",
      "=================================\n",
      "day: 5086, episode: 11\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1601109.88\n",
      "total_reward: 601109.88\n",
      "total_cost: 248650.13\n",
      "total_trades: 3393\n",
      "total_agent_reward: 601.1098757063755\n",
      "Sharpe: 0.235\n",
      "=================================\n",
      "day: 5086, episode: 12\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1433860.76\n",
      "total_reward: 433860.76\n",
      "total_cost: 251940.77\n",
      "total_trades: 3318\n",
      "total_agent_reward: 433.860763633438\n",
      "Sharpe: 0.200\n",
      "=================================\n",
      "day: 5086, episode: 13\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1626510.47\n",
      "total_reward: 626510.47\n",
      "total_cost: 256168.23\n",
      "total_trades: 3358\n",
      "total_agent_reward: 626.510466608042\n",
      "Sharpe: 0.248\n",
      "=================================\n",
      "day: 5086, episode: 14\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1654360.74\n",
      "total_reward: 654360.74\n",
      "total_cost: 256321.56\n",
      "total_trades: 3172\n",
      "total_agent_reward: 654.3607356712785\n",
      "Sharpe: 0.247\n",
      "=================================\n",
      "day: 5086, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1577852.27\n",
      "total_reward: 577852.27\n",
      "total_cost: 274789.87\n",
      "total_trades: 3351\n",
      "total_agent_reward: 577.8522706626736\n",
      "Sharpe: 0.233\n",
      "=================================\n",
      "day: 5086, episode: 16\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1657932.74\n",
      "total_reward: 657932.74\n",
      "total_cost: 284671.48\n",
      "total_trades: 3538\n",
      "total_agent_reward: 657.9327390044102\n",
      "Sharpe: 0.248\n",
      "=================================\n",
      "day: 5086, episode: 17\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1839263.73\n",
      "total_reward: 839263.73\n",
      "total_cost: 278906.59\n",
      "total_trades: 3533\n",
      "total_agent_reward: 839.2637282108906\n",
      "Sharpe: 0.285\n",
      "=================================\n",
      "day: 5086, episode: 18\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1638361.13\n",
      "total_reward: 638361.13\n",
      "total_cost: 264670.91\n",
      "total_trades: 3441\n",
      "total_agent_reward: 638.3611305745374\n",
      "Sharpe: 0.241\n",
      "=================================\n",
      "day: 5086, episode: 19\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1324505.83\n",
      "total_reward: 324505.83\n",
      "total_cost: 238054.66\n",
      "total_trades: 3198\n",
      "total_agent_reward: 324.50582650172066\n",
      "Sharpe: 0.168\n",
      "=================================\n",
      "day: 5086, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1751881.76\n",
      "total_reward: 751881.76\n",
      "total_cost: 248374.10\n",
      "total_trades: 3204\n",
      "total_agent_reward: 751.881756645276\n",
      "Sharpe: 0.272\n",
      "=================================\n",
      "day: 5086, episode: 21\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1504103.95\n",
      "total_reward: 504103.95\n",
      "total_cost: 218237.76\n",
      "total_trades: 2812\n",
      "total_agent_reward: 504.1039494409905\n",
      "Sharpe: 0.213\n",
      "=================================\n",
      "day: 5086, episode: 22\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1488710.94\n",
      "total_reward: 488710.94\n",
      "total_cost: 223484.90\n",
      "total_trades: 2684\n",
      "total_agent_reward: 488.7109359814567\n",
      "Sharpe: 0.210\n",
      "=================================\n",
      "day: 5086, episode: 23\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1441738.54\n",
      "total_reward: 441738.54\n",
      "total_cost: 232789.36\n",
      "total_trades: 2834\n",
      "total_agent_reward: 441.7385381522356\n",
      "Sharpe: 0.203\n",
      "=================================\n",
      "day: 5086, episode: 24\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1380334.30\n",
      "total_reward: 380334.30\n",
      "total_cost: 240299.70\n",
      "total_trades: 2928\n",
      "total_agent_reward: 380.3342966929443\n",
      "Sharpe: 0.186\n",
      "=================================\n",
      "day: 5086, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1648539.67\n",
      "total_reward: 648539.67\n",
      "total_cost: 279957.46\n",
      "total_trades: 3229\n",
      "total_agent_reward: 648.5396662056387\n",
      "Sharpe: 0.256\n",
      "=================================\n",
      "day: 5086, episode: 26\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1786214.39\n",
      "total_reward: 786214.39\n",
      "total_cost: 289607.21\n",
      "total_trades: 3400\n",
      "total_agent_reward: 786.2143916050538\n",
      "Sharpe: 0.295\n",
      "=================================\n",
      "day: 5086, episode: 27\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1702035.51\n",
      "total_reward: 702035.51\n",
      "total_cost: 280466.75\n",
      "total_trades: 3358\n",
      "total_agent_reward: 702.0355052296848\n",
      "Sharpe: 0.269\n",
      "=================================\n",
      "day: 5086, episode: 28\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1874867.95\n",
      "total_reward: 874867.95\n",
      "total_cost: 297956.75\n",
      "total_trades: 3406\n",
      "total_agent_reward: 874.8679466723829\n",
      "Sharpe: 0.321\n",
      "=================================\n",
      "day: 5086, episode: 29\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1577379.69\n",
      "total_reward: 577379.69\n",
      "total_cost: 285410.16\n",
      "total_trades: 3424\n",
      "total_agent_reward: 577.3796928751999\n",
      "Sharpe: 0.259\n",
      "=================================\n",
      "day: 5086, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1681574.97\n",
      "total_reward: 681574.97\n",
      "total_cost: 290879.46\n",
      "total_trades: 3460\n",
      "total_agent_reward: 681.5749742293242\n",
      "Sharpe: 0.288\n",
      "=================================\n",
      "day: 5086, episode: 31\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1631994.93\n",
      "total_reward: 631994.93\n",
      "total_cost: 309755.42\n",
      "total_trades: 3521\n",
      "total_agent_reward: 631.9949287794997\n",
      "Sharpe: 0.260\n",
      "=================================\n",
      "day: 5086, episode: 32\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1440564.91\n",
      "total_reward: 440564.91\n",
      "total_cost: 318313.60\n",
      "total_trades: 3482\n",
      "total_agent_reward: 440.56491429109633\n",
      "Sharpe: 0.202\n",
      "=================================\n",
      "day: 5086, episode: 33\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1446782.64\n",
      "total_reward: 446782.64\n",
      "total_cost: 300157.29\n",
      "total_trades: 3407\n",
      "total_agent_reward: 446.78263786378614\n",
      "Sharpe: 0.205\n",
      "=================================\n",
      "day: 5086, episode: 34\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1445950.66\n",
      "total_reward: 445950.66\n",
      "total_cost: 308629.86\n",
      "total_trades: 3485\n",
      "total_agent_reward: 445.950662743424\n",
      "Sharpe: 0.202\n",
      "=================================\n",
      "day: 5086, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1612620.95\n",
      "total_reward: 612620.95\n",
      "total_cost: 336989.88\n",
      "total_trades: 3623\n",
      "total_agent_reward: 612.6209456382055\n",
      "Sharpe: 0.252\n",
      "=================================\n",
      "day: 5086, episode: 36\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1438106.54\n",
      "total_reward: 438106.54\n",
      "total_cost: 324717.49\n",
      "total_trades: 3492\n",
      "total_agent_reward: 438.1065350826001\n",
      "Sharpe: 0.201\n",
      "=================================\n",
      "day: 5086, episode: 37\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1813199.43\n",
      "total_reward: 813199.43\n",
      "total_cost: 284469.56\n",
      "total_trades: 3271\n",
      "total_agent_reward: 813.1994283954626\n",
      "Sharpe: 0.352\n",
      "=================================\n",
      "day: 5086, episode: 38\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1868288.30\n",
      "total_reward: 868288.30\n",
      "total_cost: 279217.87\n",
      "total_trades: 3341\n",
      "total_agent_reward: 868.2883001389284\n",
      "Sharpe: 0.361\n",
      "=================================\n",
      "day: 5086, episode: 39\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1762935.72\n",
      "total_reward: 762935.72\n",
      "total_cost: 283324.90\n",
      "total_trades: 3409\n",
      "total_agent_reward: 762.9357155933994\n",
      "Sharpe: 0.310\n",
      "=================================\n",
      "day: 5086, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1749967.30\n",
      "total_reward: 749967.30\n",
      "total_cost: 271001.83\n",
      "total_trades: 3379\n",
      "total_agent_reward: 749.9673042771153\n",
      "Sharpe: 0.323\n",
      "=================================\n",
      "day: 5086, episode: 41\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1838471.34\n",
      "total_reward: 838471.34\n",
      "total_cost: 257464.94\n",
      "total_trades: 3223\n",
      "total_agent_reward: 838.4713441545525\n",
      "Sharpe: 0.340\n",
      "=================================\n",
      "day: 5086, episode: 42\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1909373.38\n",
      "total_reward: 909373.38\n",
      "total_cost: 248984.79\n",
      "total_trades: 3083\n",
      "total_agent_reward: 909.3733826846461\n",
      "Sharpe: 0.365\n",
      "=================================\n",
      "day: 5086, episode: 43\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2009284.13\n",
      "total_reward: 1009284.13\n",
      "total_cost: 223937.87\n",
      "total_trades: 2994\n",
      "total_agent_reward: 1009.2841299796225\n",
      "Sharpe: 0.407\n",
      "=================================\n",
      "day: 5086, episode: 44\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2117023.97\n",
      "total_reward: 1117023.97\n",
      "total_cost: 261283.69\n",
      "total_trades: 3259\n",
      "total_agent_reward: 1117.0239730976082\n",
      "Sharpe: 0.439\n",
      "=================================\n",
      "day: 5086, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1974485.21\n",
      "total_reward: 974485.21\n",
      "total_cost: 267247.50\n",
      "total_trades: 3240\n",
      "total_agent_reward: 974.4852123183109\n",
      "Sharpe: 0.360\n",
      "=================================\n",
      "day: 5086, episode: 46\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1898534.35\n",
      "total_reward: 898534.35\n",
      "total_cost: 271956.77\n",
      "total_trades: 3244\n",
      "total_agent_reward: 898.5343511813484\n",
      "Sharpe: 0.330\n",
      "=================================\n",
      "day: 5086, episode: 47\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1741210.10\n",
      "total_reward: 741210.10\n",
      "total_cost: 289142.61\n",
      "total_trades: 3304\n",
      "total_agent_reward: 741.2100964975102\n",
      "Sharpe: 0.278\n",
      "=================================\n",
      "day: 5086, episode: 48\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1675723.55\n",
      "total_reward: 675723.55\n",
      "total_cost: 297991.15\n",
      "total_trades: 3324\n",
      "total_agent_reward: 675.7235520761893\n",
      "Sharpe: 0.257\n",
      "=================================\n",
      "day: 5086, episode: 49\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1682116.92\n",
      "total_reward: 682116.92\n",
      "total_cost: 306198.94\n",
      "total_trades: 3407\n",
      "total_agent_reward: 682.1169236771403\n",
      "Sharpe: 0.256\n",
      "=================================\n",
      "day: 5086, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1602493.32\n",
      "total_reward: 602493.32\n",
      "total_cost: 288792.65\n",
      "total_trades: 3347\n",
      "total_agent_reward: 602.4933214286663\n",
      "Sharpe: 0.238\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adm.000\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 896, episode: 1\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1326331.40\n",
      "total_reward: 326331.40\n",
      "total_cost: 86846.76\n",
      "total_trades: 428\n",
      "total_agent_reward: 326.3313972890664\n",
      "Sharpe: 0.496\n",
      "=================================\n",
      "hit end!\n",
      "Forecasts =  True | Seed =  1 | Num episodes =  50 | Sharpe Test =  0.4961019110139536\n",
      "---\n",
      "------\n",
      "------\n",
      "LOOP NUMBER 2 --- 50 EPISODES TRAINED\n",
      "------\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adm.000\\Anaconda3\\envs\\rl_finance_py38_GPU_torch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 5086, episode: 51\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1729994.07\n",
      "total_reward: 729994.07\n",
      "total_cost: 287708.10\n",
      "total_trades: 3456\n",
      "total_agent_reward: 729.9940681318552\n",
      "Sharpe: 0.270\n",
      "=================================\n",
      "day: 5086, episode: 52\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1794978.36\n",
      "total_reward: 794978.36\n",
      "total_cost: 294863.00\n",
      "total_trades: 3598\n",
      "total_agent_reward: 794.978356724568\n",
      "Sharpe: 0.284\n",
      "=================================\n",
      "day: 5086, episode: 53\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1858982.73\n",
      "total_reward: 858982.73\n",
      "total_cost: 290912.14\n",
      "total_trades: 3612\n",
      "total_agent_reward: 858.9827319371497\n",
      "Sharpe: 0.299\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "# For any 'data' in list\n",
    "for df_name in df_names:\n",
    "\n",
    "# Data unpackage + preprocessing\n",
    "    df_main_file = path_to_datasets+f\"{df_name}.csv\"\n",
    "    df_forecasts_file = path_to_datasets+f\"{df_name+df_name_forecasts}.csv\" if (df_name_forecasts != None) else (None)\n",
    "    if encode_normalize_data:\n",
    "        train_main, valid_main, trade_main, train_forecasts, valid_forecasts, trade_forecasts = data_read_preprocessing_singleTIC_normalized_encoded(df_main_file, df_forecasts_file, **dict_args)\n",
    "    else:\n",
    "        train_main, valid_main, trade_main, train_forecasts, valid_forecasts, trade_forecasts = data_read_preprocessing_singleTIC(df_main_file, df_forecasts_file, **dict_args)\n",
    "\n",
    "# Defining features (of state_space)\n",
    "    INDICATORS_MAIN = train_main.columns.tolist()\n",
    "    for feature in unwanted_features: \n",
    "        if feature in INDICATORS_MAIN: INDICATORS_MAIN.remove(feature)\n",
    "    if df_forecasts_file != None:\n",
    "        INDICATORS_FORECASTS = train_forecasts.columns.tolist()\n",
    "        for feature in unwanted_features: \n",
    "            if feature in INDICATORS_FORECASTS: INDICATORS_FORECASTS.remove(feature)\n",
    "\n",
    "# RL Env parameters defining\n",
    "    stock_dimension = len(train_main.tic.unique())\n",
    "    state_space_main = 1 + 2*stock_dimension + len(INDICATORS_MAIN)*stock_dimension\n",
    "    state_space_forecasts = 1 + 2*stock_dimension + len(INDICATORS_FORECASTS)*stock_dimension if (df_forecasts_file != None) else (None)\n",
    "    buy_cost_list = sell_cost_list = [kwarg_buy_sell_cost] * stock_dimension\n",
    "    num_stock_shares = [0] * stock_dimension\n",
    "    print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space_main}, State Space Forecasts: {state_space_forecasts}\")\n",
    "\n",
    "    sharpe_forecastFalse = []\n",
    "    sharpe_forecastTrue = []\n",
    "\n",
    "# Main loop (for 'seeds' -> iteratively train 'data' (and if there 'data_with_forecasts') -> save each model and calculate sharpe ->  )\n",
    "    for seed_value in (seed_values*fixed_seed + (1 - fixed_seed) * [None]):\n",
    "        if fixed_seed: set_seed(seed_value)\n",
    "\n",
    "        # Training only 'data' or with 'data_with_forecasts'\n",
    "        is_forecast_list = [False, True] if (df_name_forecasts != None and (not only_forecasts_data)) \\\n",
    "                                                                        else ([False] if (not only_forecasts_data) else ([True] if (only_forecasts_data and df_name_forecasts != None) else []))\n",
    "\n",
    "        for is_forecast in is_forecast_list:\n",
    "\n",
    "            state_space = state_space_forecasts if is_forecast else state_space_main\n",
    "            INDICATORS = INDICATORS_FORECASTS if is_forecast else INDICATORS_MAIN\n",
    "            train = train_forecasts if is_forecast else train_main\n",
    "            trade = trade_forecasts if is_forecast else trade_main\n",
    "            if valid_split: valid = valid_forecasts if is_forecast else valid_main\n",
    "\n",
    "            # Define 'env' parapeters \n",
    "            env_kwargs = env_kwargs_reinit()\n",
    "\n",
    "            # Counter for training (if k=0 -> no model, initialize) ELSE (model = trained)\n",
    "            k=0\n",
    "\n",
    "            # Iterative training loop\n",
    "            for i in range(start_training_episode,end_training_episode,step_training_episodes):\n",
    "                for g in range(2):print('------')\n",
    "                print(f'LOOP NUMBER {int(i/step_training_episodes)} --- {i-start_training_episode} EPISODES TRAINED')\n",
    "                for g in range(2):print('------')\n",
    "                NUM_EPISODES = step_training_episodes \n",
    "                EPISODE_LENGTH = len(train)\n",
    "                NUM_TRAINING_STEPS_FOR_1_TRIAL = NUM_EPISODES * EPISODE_LENGTH \n",
    "\n",
    "                model_folder_name = f\"{df_name}__{str(df_name_forecasts)[1:]*(is_forecast) + 'without_forecast'*(1-is_forecast)}__{valid_split*'Valid' + (1 - valid_split)*'Test'}{int(test_and_valid_pct*100)}__NormEncd{encode_normalize_data}\"\n",
    "                model_file_name = f\"{model_name.upper()}__iterationEp{step_training_episodes}__trainedEp{i}__lr{algorithm_parameters['learning_rate']}__seed{str(seed_value)*fixed_seed + str(None)*(1-fixed_seed)}\"\n",
    "                #name_of_the_save_file = f\"{path_to_models}{model_name.upper()}_NormEnc{encode_normalize_data}_techindicators{dict_args['tech_indicators_usage']}_vix{dict_args['use_vix']}_turbulence{dict_args['use_turbulence']}\\\\{df_name}_{model_name}_lr{algorithm_parameters['learning_rate']}_Forecast{is_forecast}_Seed{str(seed_value)*fixed_seed + str(None)*(1-fixed_seed)}_Episodes{i}_Step{step_training_episodes}_Sharpe{valid_split*'Valid' + (1 - valid_split)*'Test'}{int(test_and_valid_pct*100)}\"\n",
    "                name_of_the_save_file = path_to_models + model_folder_name + f\"\\\\{model_file_name}\"\n",
    "                name_of_the_save_file_zip = name_of_the_save_file + \".zip\"\n",
    "\n",
    "                # MODEL TRAINING\n",
    "                if not os.path.exists(name_of_the_save_file_zip):\n",
    "                    \n",
    "                    # Initialize env (correcting number of episodes passed)\n",
    "                    e_train_gym, env_train = env_reinit(train, env_kwargs_reinit())\n",
    "                    e_train_gym.episode = i-start_training_episode + 1\n",
    "                    \n",
    "                    # Callback\n",
    "                    if chosen_callback == 'eval':\n",
    "                        eval_freq = EPISODE_LENGTH * eval_freq_ep\n",
    "                        callback_arg = EvalCallback(eval_env=e_train_gym, eval_freq=eval_freq, n_eval_episodes=n_eval_episodes, best_model_save_path=name_of_the_save_file+'\\\\eval_callback', log_path=name_of_the_save_file+'\\\\eval_callback', deterministic=True)\n",
    "                    elif chosen_callback == 'tensorboard':\n",
    "                        callback_arg = TensorboardCallback(BaseCallback)\n",
    "                    \n",
    "                    # MODEL TRAINING\n",
    "                    #e_train_gym, env_train = env_reinit(train, env_kwargs_reinit())\n",
    "                    agent = DRLAgent(env = env_train)\n",
    "                    model = agent.get_model(model_name=model_name, model_kwargs = algorithm_parameters, verbose=0, seed=seed_value) if (k == 0) else model.load(prev_name, env = env_train)\n",
    "                    trained = agent.train_model(model=model, \n",
    "                                                    tb_log_name=model_name,\n",
    "                                                    total_timesteps=NUM_TRAINING_STEPS_FOR_1_TRIAL,\n",
    "                                                    callback=callback_arg)\n",
    "\n",
    "                    # CALCULATING sharpe on 'valid / test'\n",
    "                    env_trade_gym, env_trade = env_reinit(valid, env_kwargs_reinit()) if valid_split else env_reinit(trade, env_kwargs_reinit())\n",
    "                    df_account_value, df_actions = DRLAgent.prediction(model=trained, environment = env_trade_gym)\n",
    "                    sharpe = calculate_sharpe(df_account_value)\n",
    "                    sharpe_forecastTrue.append(sharpe) if is_forecast else sharpe_forecastFalse.append(sharpe)\n",
    "\n",
    "                    # MODEL SAVING\n",
    "                    trained.save(name_of_the_save_file_zip)\n",
    "\n",
    "                    prev_name = name_of_the_save_file_zip\n",
    "                    print('Forecasts = ', is_forecast, '| Seed = ', seed_value, '| Num episodes = ', i, f'| Sharpe {valid_split*\"Valid\" + (1 - valid_split)*\"Test\"} = ', sharpe)\n",
    "                    print('---')\n",
    "                    k+=1\n",
    "\n",
    "                # MODEL LOADING + sharpe calculating\n",
    "                else:\n",
    "                    print('Model already exists')\n",
    "                    k+=1\n",
    "\n",
    "                    e_train_gym, env_train = env_reinit(train, env_kwargs_reinit())\n",
    "                    agent = DRLAgent(env = env_train)\n",
    "                    model = agent.get_model(model_name=model_name, model_kwargs = algorithm_parameters, verbose=0, seed=seed_value)\n",
    "                    trained = model.load(name_of_the_save_file, env = env_train)\n",
    "\n",
    "                    env_trade_gym, env_trade = env_reinit(valid, env_kwargs_reinit()) if valid_split else env_reinit(trade, env_kwargs_reinit())\n",
    "                    df_account_value, df_actions = DRLAgent.prediction(model=trained, environment = env_trade_gym)\n",
    "                    sharpe = calculate_sharpe(df_account_value)\n",
    "                    sharpe_forecastTrue.append(sharpe) if is_forecast else sharpe_forecastFalse.append(sharpe)\n",
    "                    \n",
    "                    prev_name = name_of_the_save_file_zip\n",
    "                    print('Forecasts = ', is_forecast, '| Seed = ', seed_value, '| Num episodes = ', i, f'| Sharpe {valid_split*\"Valid\" + (1 - valid_split)*\"Test\"} = ', sharpe)\n",
    "                    print('---')\n",
    "\n",
    "\n",
    "        print('------')\n",
    "        print(df_name)\n",
    "        print('Seed average no forecast = ', np.mean(sharpe_forecastFalse))\n",
    "        print('Seed average with forecast = ', np.mean(sharpe_forecastTrue))\n",
    "        print('------')\n",
    "        print('No forecast = ', sharpe_forecastFalse)\n",
    "        print('With forecast =', sharpe_forecastTrue)\n",
    "        print('------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
